from django.shortcuts import render, redirect
from django.http import JsonResponse, HttpResponse
from datetime import datetime
import requests
from langchain_core.messages import HumanMessage, AIMessage
from django.views.decorators.csrf import csrf_exempt
import json
from openai import OpenAI
from django.conf import settings
import os
from langchain_openai import ChatOpenAI  # ìµœì‹  LangChainìš© OpenAI ì—°ë™
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.docstore.document import Document
from langchain.docstore import InMemoryDocstore
import faiss

# Create your views here.
def chat_view(request):
    return render(request, 'homepage.html')

def calculator_view(request):
    return render(request, 'calculator.html')

def redirect_to_main(request):
    return redirect('/main')

def load_faiss_index():
    # í˜„ì¬ íŒŒì¼ (views.py) ê¸°ì¤€ìœ¼ë¡œ faiss í´ë” ê²½ë¡œ ê³„ì‚°
    base_dir = os.path.dirname(__file__)              # â†’ main/
    faiss_dir = os.path.join(base_dir, "faiss")       # â†’ main/faiss

    # docs.json ë¡œë“œ
    docs_path = os.path.join(faiss_dir, "docs.json")
    with open(docs_path, "r", encoding="utf-8") as f:
        texts = json.load(f)

    docs = [Document(page_content=text) for text in texts]

    # FAISS index ë¡œë“œ
    index_path = os.path.join(faiss_dir, "my_index.index")
    index = faiss.read_index(index_path)

    # ì„ë² ë”© ëª¨ë¸
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # LangChainìš© docstore ë° mapping
    index_to_docstore_id = {i: str(i) for i in range(len(docs))}
    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})

    return FAISS(embedding_model.embed_query, index, docstore, index_to_docstore_id)

vectorstore = load_faiss_index()


client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def chat_api(user_input, chat_history=None):
    if chat_history is None:
        chat_history = []

    # ğŸ” OpenAI LLM ì„¤ì •
    llm = ChatOpenAI(
        model="gpt-4o",  # ë˜ëŠ” gpt-3.5-turbo
        temperature=0.7,
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    contextualize_q_prompt = PromptTemplate(
        input_variables=["input", "chat_history"],
        template="""
        ê¸°ì¡´ ëŒ€í™”:
        {chat_history}

        ì§ˆë¬¸:
        {input}

        ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”:
        """
    )

    qa_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ëŒ€ì¶œ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    ("system", "ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:\n\n{context}")
    ])


    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    response = rag_chain.invoke({
        "input": user_input,
        "chat_history": chat_history
    })

    chat_history.extend([
        HumanMessage(content=user_input),
        AIMessage(content=response["answer"])
    ])

    return response["answer"], chat_history


@csrf_exempt
def chat_api_view(request):
    if request.method == 'POST':
        try:
            body = json.loads(request.body)
            user_input = body.get('message', '')

            # ì„¸ì…˜ ê¸°ë°˜ ëŒ€í™” ì´ë ¥ ë¶ˆëŸ¬ì˜¤ê¸°
            raw_history = request.session.get('chat_history', [])
            formatted_history = []
            for i in range(0, len(raw_history), 2):
                formatted_history.append(HumanMessage(content=raw_history[i]))
                if i + 1 < len(raw_history):
                    formatted_history.append(AIMessage(content=raw_history[i+1]))

            # LangChain ê¸°ë°˜ ë‹µë³€ ìƒì„±
            answer, updated_history = chat_api(user_input, formatted_history)

            # ìƒˆ íˆìŠ¤í† ë¦¬ ì„¸ì…˜ì— ì €ì¥ (ë‚´ìš©ë§Œ ì €ì¥)
            request.session['chat_history'] = [m.content for m in updated_history]

            return JsonResponse({'reply': answer})
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)

    return JsonResponse({'error': 'Invalid method'}, status=405)

def download_chat_log(request):
    # ì„¸ì…˜ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
    raw_history = request.session.get('chat_history', [])

    # í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì§ìˆ˜: user, í™€ìˆ˜: AI)
    lines = []
    for i in range(0, len(raw_history), 2):
        user_msg = raw_history[i]
        lines.append(f"ì‚¬ìš©ì: {user_msg}")
        if i + 1 < len(raw_history):
            ai_msg = raw_history[i + 1]
            lines.append(f"AI: {ai_msg}")
        lines.append("")  # ì¤„ë°”ê¿ˆ

    chat_text = "\n".join(lines)

    # íŒŒì¼ëª… ë§Œë“¤ê¸°
    filename = f"chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

    # ì‘ë‹µìœ¼ë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ ë°˜í™˜
    response = HttpResponse(chat_text, content_type='text/plain')
    response['Content-Disposition'] = f'attachment; filename="{filename}"'
    return response






# === runpod api íê¸° ===
# def ask_from_runpod(message, history):
#     response = requests.post(
#         "194.68.245.203:22103/chat",
#         json={
#             "question": message,
#             "history": [msg.dict() for msg in history]  # LangChain message í¬ë§·ì„ dictë¡œ ë³€í™˜
#         }
#     )
#     print("ğŸ“¡ RunPod ì‘ë‹µ ìƒíƒœì½”ë“œ:", response.status_code)
#     print("ğŸ“„ RunPod ì‘ë‹µ í…ìŠ¤íŠ¸:", response.text)
#     data = response.json()
#     print(data)
#     return data["answer"], [HumanMessage(**m) if m["type"] == "human" else AIMessage(**m) for m in data["history"]]



# import traceback

# @csrf_exempt
# def chat_api(request):
#     if request.method == 'POST':
#         try:
#             body = json.loads(request.body)
#             print("ë°›ì€ ë©”ì‹œì§€:", body)

#             user_input = body.get('message')
#             print("user_input:", user_input)

#             raw_history = request.session.get('chat_history', [])
#             formatted_history = []
#             for i in range(0, len(raw_history), 2):
#                 formatted_history.append(HumanMessage(content=raw_history[i]))
#                 if i + 1 < len(raw_history):
#                     formatted_history.append(AIMessage(content=raw_history[i+1]))

#             answer, updated_history = ask_from_runpod(user_input, formatted_history)
#             request.session['chat_history'] = [msg.content for msg in updated_history]

#             return JsonResponse({'reply': answer})

#         except Exception as e:
#             print("ì—ëŸ¬ ë°œìƒ:", str(e))
#             traceback.print_exc()  # âœ… êµ¬ì²´ì ì¸ ì—ëŸ¬ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
#             return JsonResponse({'error': str(e)}, status=500)

#     return JsonResponse({'error': 'Invalid method'}, status=405)


